{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9abf6f5f-3396-4285-89ec-4b1709f6553c",
   "metadata": {},
   "source": [
    "# MobileBERT for Question Answering on the SQuAD dataset\n",
    "\n",
    "### 4. Creating a Gradio app to deploy the model \n",
    "\n",
    "In these notebooks we are going use [MobileBERT implemented by HuggingFace](https://huggingface.co/docs/transformers/model_doc/mobilebert) on the question answering task by text-extraction on the [The Stanford Question Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer/). The data is composed by a set of questions and paragraphs that contain the answers. The model will be trained to locate the answer in the context by giving the positions where the answer starts and ends.\n",
    "\n",
    "In this notebook we are going to create [Pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines) and deploy it with a [Gradio](https://huggingface.co/gradio) app.\n",
    "\n",
    "More info from HuggingFace docs:\n",
    "- [Question Answering](https://huggingface.co/tasks/question-answering)\n",
    "- [Glossary](https://huggingface.co/transformers/glossary.html#model-inputs)\n",
    "- [Question Answering chapter of NLP course](https://huggingface.co/learn/nlp-course/chapter7/7?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a9fef8-4780-4779-89de-662eb014d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import Pipeline\n",
    "from transformers import AutoTokenizer, MobileBertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556b270e-e5c4-4bbf-9025-e151db07071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileBERTQAPipeline(Pipeline):\n",
    "    def __init__(self, hf_model_checkpoint):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(hf_model_checkpoint)\n",
    "        model = MobileBertForQuestionAnswering.from_pretrained(hf_model_checkpoint)\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        # load finetuned-model\n",
    "        model.load_state_dict(\n",
    "        torch.load('mobilebertqa_ft',\n",
    "               map_location=torch.device('cpu'))\n",
    "        )\n",
    "\n",
    "        super().__init__(model, tokenizer)\n",
    "\n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        return {}, {}, {}\n",
    "\n",
    "    def preprocess(self, inputs, maybe_arg=2):\n",
    "        model_input = self.tokenizer(*inputs, return_tensors=\"pt\")\n",
    "        self.context_tokens = model_input['input_ids']\n",
    "        return model_input\n",
    "\n",
    "    def _forward(self, model_inputs):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**model_inputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        start_probs = F.softmax(model_outputs.start_logits, dim=-1)[0]\n",
    "        end_probs   = F.softmax(model_outputs.end_logits,   dim=-1)[0]\n",
    "\n",
    "        # find the max class that the softmax gives\n",
    "        start = torch.argmax(start_probs).item()\n",
    "        end = torch.argmax(end_probs).item()\n",
    "        \n",
    "        # predicted answer\n",
    "        answer_tokens = self.context_tokens[0][start:end]\n",
    "        answer_text = self.tokenizer.decode(answer_tokens, skip_special_tokens=True,\n",
    "                                            clean_up_tokenization_spaces=True)\n",
    "\n",
    "        # start position in text\n",
    "        start_text = len(self.tokenizer.decode(self.context_tokens[0][:start], skip_special_tokens=True,\n",
    "                                               clean_up_tokenization_spaces=True)) + 1\n",
    "\n",
    "        before_answer = self.tokenizer.decode(self.context_tokens[0], skip_special_tokens=True,\n",
    "                                               clean_up_tokenization_spaces=True)[:start_text]\n",
    "\n",
    "        return {'start': start_text,\n",
    "                'end': start_text + len(answer_text),\n",
    "                'answer': answer_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d675913-0ea4-4568-b325-c528b13477c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipeline = MobileBERTQAPipeline('google/mobilebert-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ba72c3-a993-479f-afa6-aec5da79c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_answer(question, paragraph,\n",
    "                     tokenizer=AutoTokenizer.from_pretrained('google/mobilebert-uncased')):\n",
    "\n",
    "    # Rewrite the paragraph in the way the tokenizer sees it\n",
    "    # otherwise there may be mismatches between the 'raw' input\n",
    "    # text and the decode one\n",
    "    tokens = tokenizer(paragraph)['input_ids']\n",
    "    paragraph = tokenizer.decode(tokens,\n",
    "                                 skip_special_tokens=True,\n",
    "                                 clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # Use the Hugging Face pipeline to get the answer\n",
    "    answer = qa_pipeline((paragraph, question))\n",
    "\n",
    "    # Extract start and end indices from the pipeline output\n",
    "    start_index = answer['start']\n",
    "    end_index = answer['end']\n",
    "\n",
    "    # Highlight the answer within the paragraph\n",
    "    highlighted_text = (paragraph[:start_index] + \n",
    "                        \"<span style='background-color: #FFA500;'> <b>\" + \n",
    "                        paragraph[start_index:end_index] + \n",
    "                        \"</b> </span>\" + \n",
    "                        paragraph[end_index:])\n",
    "\n",
    "    return highlighted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95087352-1860-416c-806b-19ea074cc8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.Interface(\n",
    "    fn=highlight_answer,\n",
    "    inputs=[\"text\", \"text\"],\n",
    "    outputs=\"html\",\n",
    "    title=\"Highlight Answer in Paragraph\",\n",
    "    description=\"Highlight the answer within the paragraph\",\n",
    "    examples=[\n",
    "        [\"What is the quick animal?\", \"The quick brown fox jumps over the lazy dog.\"],\n",
    "        [\"What color is the sky?\", \"The sky is blue.\"]\n",
    "    ]\n",
    ").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2024",
   "language": "python",
   "name": "ml2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
