{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OY3VPd2kvIMq"
   },
   "source": [
    "# Transfer Learning with VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vAlTNJgsuO3Z"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as  F\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, models\n",
    "import torchvision.transforms.v2 as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gjhq-kB7xkBU"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YZwej3HUuVDv"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LjxQeROY1nrV"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG16 is a CNN for image classification, the runner-up in the [ImageNet]() competition in 2015. A pre-trained VGG16 model can be easily downloaded from `torchvision.models`, with the `DEFAULT` pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(weights='DEFAULT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the model will display the model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Have a look at the model. You should be able to recognise almost all the layers of this CNN. What `AdaptiveAvgPool2d` does? Why it is useful?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the full model is composed of two `nn.Sequential` sub-modules: `features` and `classifier`. The `features` sub-module is composed of convolutional layers and acts as a feature extraction module. The `classifier` sub-module is composed of linear layers and acts as a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using transfer learning, we are interested in tuning the model for classification of flowers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JmToWYgFvzfs"
   },
   "source": [
    "## Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we use the [tf_flowers](https://www.tensorflow.org/datasets/catalog/tf_flowers) data set. Extracting the data will take a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ -d ./data/flower_photos ]; then\n",
    "  echo \"Directory exists\"\n",
    "else\n",
    "    wget http://download.tensorflow.org/example_images/flower_photos.tgz -P ./data/\n",
    "    cd data/ && tar xfz flower_photos.tgz\n",
    "    rm -f flower_photos/LICENSE.txt\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set consists of 3670 images of 5 flower classes: daisy, dandelion, roses, sunflowers, and tulips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Check if any of these classes were already present in the original ImageNet data set, on which VGG-16 has been trained.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Read [PyTorch documentation for VGG16]() and determine the mean and standard deviation needed to normalise the images, as well as the size of the input images. Define a transformation that `RandomResizedCrop` the input images to the correct size, and normalise them. Use the `ToImage()` transform to convert images to tensors, and use `ToDtype(torch.float32, scale=True)` to convert the data for `float32` before normalisation.* `RandomResizeCrop` allows to crop the images to the correct input size, while adding a bit of data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "mean =\n",
    "std =\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    # RandomResizeCrop to the correct size for VGG-16\n",
    "    # TODO\n",
    "    \n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    # Normalize input\n",
    "    # TODO\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The data set is structured in sub-folders named after the classes of flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = f\"{os.getcwd()}/data/flower_photos\"\n",
    "print(dataroot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -l data/flower_photos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The [`ImageFolder`](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder) class allows to easily load such dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(dataroot, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of data in this dataset is not very large, therefore transfer learning is a very convenient technique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Number of images: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the data set into a training set and a validation set. For simplicity, we skip the creation of a separate test set (something you shouldn't do!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(dataset)\n",
    "idx_train, idx_valid = train_test_split(np.arange(n), test_size=0.2, random_state=42)\n",
    "\n",
    "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(idx_train)\n",
    "valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(idx_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create a function doing all of the above, for convenient re-use.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(batch_size, dataroot=\"data/flower_photos\"):\n",
    "    # Define mean and standard deviation for normalisation\n",
    "    # TODO\n",
    "    mean =\n",
    "    std =\n",
    "\n",
    "    # Define the transform\n",
    "    # TODO\n",
    "    transform = transforms.Compose([\n",
    "        \n",
    "    ])\n",
    "    \n",
    "    dataset = datasets.ImageFolder(dataroot, transform=transform)\n",
    "    \n",
    "    n = len(dataset)\n",
    "    idx_train, idx_valid = train_test_split(np.arange(n), test_size=0.2, random_state=42)\n",
    "\n",
    "    train_sampler = torch.utils.data.sampler.SubsetRandomSampler(idx_train)\n",
    "    valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(idx_valid)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    validloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return trainloader, validloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GwtCZcXr15Hz"
   },
   "source": [
    "#### Visualizing Images and Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8EnlxbtI2ekB"
   },
   "source": [
    "First, let's define a dictionary mapping labels (numbers from 0 to 4 denoting one of the flower classes) to the acutal classes names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9-sD0b91158E",
    "outputId": "f62e2468-d7a2-4666-e4ff-6678ce4f714a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
    "\n",
    "label_to_name = { \n",
    "    i : name \n",
    "    for i, name in enumerate(classes) \n",
    "}\n",
    "\n",
    "print(label_to_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3XeljxpXBdTP"
   },
   "source": [
    "Then we can visualise a batch of images. PyTorch stores images with in the `C x H x W` convention (where C is number of channels, H the image height and W the image width) while `matplotlib` uses the `H x W x C` convention. This means that we have to transpose our tensor from `C x H x W` to  `H x W x C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 873
    },
    "colab_type": "code",
    "id": "jv4sS5_x3HEI",
    "outputId": "8e211f80-c25f-4c60-d679-405af5536d17",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainiter = iter(trainloader)\n",
    "images, labels = next(trainiter)\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "for idx in range(64):\n",
    "    ax = fig.add_subplot(8, 8, idx + 1, xticks=[], yticks=[])\n",
    "    \n",
    "    img = images[idx].numpy().squeeze()\n",
    "    \n",
    "    for i in range(3):\n",
    "        img[i,:,:] = img[i,:,:] * std[i] + mean[i]\n",
    "\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    \n",
    "    name = label_to_name[labels[idx].item()]\n",
    "    \n",
    "    ax.set_title(name, fontdict={\"fontsize\": 12})\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run the previous cell a few times. How does the dataset looks like?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JuPRGLlc4792"
   },
   "source": [
    "## Adapting Pre-Trained VGG-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already downloaded the VGG16 model from `torchvision.models` above, with `DEFAULT` pre-trained weights. In order to use this model for classification of flowers, we need two things:\n",
    "* Freeze the model parameters of the layers we do not want to train\n",
    "* Replace the last fully connected layer with a layer with the correct number of output classes (`5`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VGG16 architecture is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create a function taking an integer `layer_size` that:*\n",
    "* _Defines a VGG16 model with pre-trained weights_\n",
    "* _Freezes the weights of all layers_\n",
    "* _Substitutes the last two pre-trained linear layers with new (untrained) linear layers_\n",
    "\n",
    "*The last linear layer needs the number of classes as output, while the input of the last linear layer should be parametrized by `layer_size`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(layer_size):\n",
    "    model = models.vgg16(weights='DEFAULT')\n",
    "    \n",
    "    # Freeze all model parameters\n",
    "    # TODO\n",
    "    \n",
    "    # Substitute the last two linear layers \n",
    "    # TODO\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the `create_model` function works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JV0MVFt0_YPH"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nk6Kug58_apy"
   },
   "source": [
    "*Complete the training function defined below, assuming that the `params` dictionary contains the following entries:*\n",
    "* `layer_size`\n",
    "* `batch_size`\n",
    "* `lr` _(learning rate)_\n",
    "* `n_epochs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8RxOwQhZpjma"
   },
   "source": [
    "Finally we can train our network as usual. The `require_grad=False` parameter for the frozen layers will prevent the optimiser to change the weights and biases of those layer. Effectively, only the last linear layer we modified will be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ht5oyxmh_bwB"
   },
   "outputs": [],
   "source": [
    "def train_fn(params, dataroot=\"data/flower_photos\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Create a model using the \"create_model\" function\n",
    "    # TODO\n",
    "    model =\n",
    "     \n",
    "    model.to(device)\n",
    "    \n",
    "    # Create data loaders using the \"create_dataset\" function\n",
    "    # TODO\n",
    "    trainloader, validloader =\n",
    "    \n",
    "    # Define an appropriate loss function\n",
    "    # TODO\n",
    "    loss_function =\n",
    "    \n",
    "    # Define an Adam optimizer\n",
    "    # TODO\n",
    "    optimizer =\n",
    "    \n",
    "    best_valid_loss = np.inf\n",
    "    best_accuracy = np.inf\n",
    "    \n",
    "    pbar = trange(params[\"n_epochs\"], desc='Training', leave=True)\n",
    "    for epoch in pbar:\n",
    "        epoch_loss = 0\n",
    "\n",
    "        # Ensure model is in training mode\n",
    "        # TODO\n",
    "        \n",
    "        for images, labels in trainloader:\n",
    "\n",
    "            # Move data to GPU\n",
    "            # TODO\n",
    "            images, labels =\n",
    "        \n",
    "            # Initialize optimizer gradients to zero\n",
    "            # TODO\n",
    "            \n",
    "            # Perform forward pass\n",
    "            # TODO\n",
    "            \n",
    "            # Compute the loss\n",
    "            # TODO\n",
    "            \n",
    "            # Perform backpropagation\n",
    "            # TODO\n",
    "            \n",
    "            # Update model weights\n",
    "            # TODO\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        valid_loss = 0\n",
    "        accuracy = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Ensure model is in evaluation mode\n",
    "            # TODO\n",
    "\n",
    "            for images, labels in validloader:\n",
    "\n",
    "                # Move data to GPU\n",
    "                # TODO\n",
    "                images, labels =\n",
    "                    \n",
    "                # Perform forward pass\n",
    "                # TODO\n",
    "                    \n",
    "                # Compute the loss\n",
    "                valid_loss += loss_function(output, labels).item()\n",
    "                    \n",
    "                # Compute class probabilities\n",
    "                # TODO\n",
    "                p =\n",
    "                \n",
    "                # Compute accuracy\n",
    "                top_p, top_c = p.topk(1, dim=1) # Top prediction\n",
    "                equals = (top_c == labels.view(*top_c.shape)).type(torch.FloatTensor)\n",
    "                accuracy += torch.mean(equals)\n",
    "                    \n",
    "        t_loss = epoch_loss/len(trainloader)\n",
    "        v_loss = valid_loss/len(validloader)\n",
    "        acc = accuracy.item()/len(validloader)*100\n",
    "        \n",
    "        # Store best model (perform a deep copy of the state dictionary)\n",
    "        # Store best accuracy in \"best_accuracy\"\n",
    "        # TODO \n",
    "            \n",
    "        pbar.set_postfix({\"Accuracy\": acc})\n",
    "            \n",
    "    # Load best model at the end of training\n",
    "    # TODO\n",
    "    model.load_state_dict(best_state_dict)\n",
    "            \n",
    "    return model.eval(), best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train with the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"layer_size\": 1024,\n",
    "    \"lr\": 0.005,\n",
    "    \"batch_size\": 128,\n",
    "    \"n_epochs\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, acc = train_fn(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you finish early, you can try to re-run the notebook whit less frozen parameters and study the impact on accuracy and training time. If time allows it, play around with the hyperparameters of the model and see if you can obtain a better accuracy on the validation set.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebooks has been structured in such a way that the training function is self-contained. This allows to adapt it to a framework for hyperparameter tuning.*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "01_CIFAR10-CNN-simple.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
