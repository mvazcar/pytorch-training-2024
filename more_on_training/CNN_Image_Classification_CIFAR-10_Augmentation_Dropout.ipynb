{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81546d97-7f3c-4b32-b12d-b15543a33d13",
   "metadata": {},
   "source": [
    "# CNN for Image Classification (CIFAR-10): Data Augmentation and Dropout Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999d1b14-8ad8-4c0a-91bb-28c629ae235e",
   "metadata": {},
   "source": [
    "In this exercise we are going to implement and train a convolutional neural network (CNN) for classifying images in the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) data set using two regularisation techniques: data agumentation and dropout layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f92233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2831f0-8ded-4df4-835b-9805fbbdb510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms.v2 as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beb5e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f07ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0158df0-ed5a-45f4-9aa5-531a50665e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028b4629-f53e-4ebe-8faa-7fb2c0633e9a",
   "metadata": {},
   "source": [
    "## Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bf5dca",
   "metadata": {},
   "source": [
    "### Loading the Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108b4304-30f7-4d2a-a57a-5075b63178d8",
   "metadata": {},
   "source": [
    "**Note**: in constrast to the previous notebook, here we loaded `transforms.v2`, the new version of `transforms`. See [V1 or V2? Which one should I use?](https://pytorch.org/vision/stable/transforms.html#v1-or-v2-which-one-should-i-use) for examples and a short explanation of the difference.  You can try out V2, or stick with V1. For this exercise, both are equivalent. If you decide to use V1 as in the previous notebook, change the above import `import torchvision.transforms.v2 as transforms` to `from torchvision import transforms` (and re-run the cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fb0301-a4bc-45b1-8070-df54a9f14313",
   "metadata": {},
   "source": [
    "The data in [`torchvision`](https://pytorch.org/vision/stable/index.html) data sets consists of [PIL](https://pillow.readthedocs.io/en/stable/reference/Image.html) images with values in the interval $[0, 1]$. In order to use the images for training, they need to be converted to PyTorch tensors ([`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html)). It is also good practice to normalise the images on the interval $[-1, 1]$ to reduce data skewness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562aa90c",
   "metadata": {},
   "source": [
    "However, we want to perform data augmentation on our data set, in order to reduce overfitting. In addition to the `ToImage` and `Normalize` transformations we already used, `torchvision.transforms` contains many more transformation for data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb80a89-946c-4c25-bd12-cceb7cbac1c8",
   "metadata": {},
   "source": [
    "*Define a transformation, using [`torchvision.transforms`](https://pytorch.org/vision/stable/transforms.html#v2-api-reference-recommended), converting PIL images to PyTorch tensors and normalising them (with mean and standard deviation of `0.5` for each channel). Additionally, apply the following data agumentation transformations:*\n",
    "* _Random horizontal flip_\n",
    "* _Random affine transformation (rotation + translation)_\n",
    "    * _10 degrees rotation maximum_\n",
    "    * _0.1 translation maximum (in each direction)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bb5da8-b7f4-4224-8c8c-253452b99d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_augmentation = transforms.Compose([\n",
    "    # TODO\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00cf932",
   "metadata": {},
   "source": [
    "Using the transformation defined above, we can define and download the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ad8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "augtrainset = datasets.CIFAR10(\"data/\", download=True, train=True, transform=transform_augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114aabae",
   "metadata": {},
   "source": [
    "The test set does not need data augmentation. We also want to define a training set without data agumentation for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca15cc7-fa69-4e94-a220-96ace6361513",
   "metadata": {},
   "source": [
    "*Define a transformation, using [`torchvision.transforms`](https://pytorch.org/vision/stable/transforms.html#v2-api-reference-recommended), converting PIL images to PyTorch tensors and normalising them (with mean and standard deviation of `0.5` for each channel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f852f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # TODO\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3a5d03-6cbb-4395-b203-34403fc8af05",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.CIFAR10(\"data/\", download=True, train=True, transform=transform)\n",
    "testset = datasets.CIFAR10(\"data/\", download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35d57f1",
   "metadata": {},
   "source": [
    "*Use [scikit-learn](https://scikit-learn.org/stable/)'s `train_test_split` to define which images (i.e. which indices) of the training set are used for training and which are used for validation, using random sampling. Use 20% of training data for the validation set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e457ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(trainset)\n",
    "\n",
    "# TODO\n",
    "idx_train, idx_valid ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54a0d5f",
   "metadata": {},
   "source": [
    "Given the indices of the training and validation sets obtained above, it is possible to define PyTorch random data samplers as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0f8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(idx_train)\n",
    "valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(idx_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5367d",
   "metadata": {},
   "source": [
    "Define a data loader (using a data set and a sampler) for the augmented and non-aurgmented training sets, as well as the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792cc433",
   "metadata": {},
   "outputs": [],
   "source": [
    "augtrainloader = torch.utils.data.DataLoader(augtrainset, batch_size=64, sampler=train_sampler, drop_last=True, num_workers=2)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, sampler=train_sampler, drop_last=True, num_workers=2)\n",
    "validloader = torch.utils.data.DataLoader(trainset, batch_size=64, sampler=valid_sampler, drop_last=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def1589f",
   "metadata": {},
   "source": [
    "Here we chose a batch size of `64`. The batch size can have a big impact on training and therefore it is an hyperparameter of the model. We drop the last batch so that all batches have the same size, for simplicity. For the test set, we just load the examples in sequence, with the same batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5e9850",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, drop_last=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76be6be",
   "metadata": {},
   "source": [
    "Finally, we can define iterators to iterate over `trainloader`, `trainloader_aug`, `validloader` and `testloader` batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7517297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "augtrainiter = iter(augtrainloader)\n",
    "trainiter = iter(trainloader)\n",
    "validiter = iter(validloader)\n",
    "testiter = iter(testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a052938",
   "metadata": {},
   "source": [
    "### Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15e4e47",
   "metadata": {},
   "source": [
    "It is always good practice to look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b818e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_name = { \n",
    "    i : name \n",
    "    for i, name in enumerate([\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19f2fcd-4670-4397-862b-b1bd5183011c",
   "metadata": {},
   "source": [
    "*Get a batch of images with data augmentation and visualize them.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21146a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "images, labels =\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "for idx in range(64):\n",
    "    ax = fig.add_subplot(8, 8, idx + 1, xticks=[], yticks=[])\n",
    "    \n",
    "    # Un-normalize image\n",
    "    img = images[idx].numpy().squeeze() * 0.5 + 0.5 \n",
    "\n",
    "    # Transpose image from C x H x W to H x W x C\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    \n",
    "    # Plot image\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    # Get name\n",
    "    name = label_to_name[labels[idx].item()]\n",
    "    \n",
    "    ax.set_title(name, fontdict={\"fontsize\": 12})\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78831578",
   "metadata": {},
   "source": [
    "We can see that rotations and translations are indeed applied to the images used for training.However, this introduced empty pixels. If we had bigger images, we could have cropped them after the affine transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a262afc0",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network with Dropout Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fc8da9",
   "metadata": {},
   "source": [
    "*Define a simple convolutional neural network with the following characteristics:*\n",
    "* _Three convolutional layers with `16`, `32`, and `64` channels with $3 \\times 3$ kernels_\n",
    "* _Maximum pooling with $2 \\times 2$ kernel_\n",
    "* _Two fully connected layers for classification_\n",
    "* _Rectified linear unit (ReLU) activation function (after each convolutional layer and after the first fully connected layer_\n",
    "* _Dropout layers with probability $p$ (defined as input of the model constructor), before the first and second linear layers_\n",
    "\n",
    "*Decide if you want to add a `LogSoftmax` layer (or, equivalently, `F.log_softmax`) at the end of the network or not. You'll have to adjust the loss function accordingly.*\n",
    "\n",
    "*Choose appropriate values for padding, and for the input and output size of the fully connected layers. You can consider that the hard-coded batch size of `64` defined above does not change.*\n",
    "\n",
    "*To simplify the `forward()` function compared to the previous notebook, you can define the convolutional part and the fully connected part as `nn.Sequential` (sub-)modules.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa3b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, dropoutp=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            # TODO\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            # TODO\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Input shape\n",
    "        assert x.shape == (64, 3, 32, 32)\n",
    "\n",
    "        # Apply convolutional layers\n",
    "        # TODO\n",
    "        x =\n",
    "\n",
    "        # Flatten features for fully connected layers\n",
    "        # TODO\n",
    "        x =\n",
    "\n",
    "        assert x.shape == (64, 64 * 4 * 4)\n",
    "\n",
    "        # Apply fully connected layers\n",
    "        # TODO\n",
    "        x =\n",
    "\n",
    "        assert x.shape == (64, 10)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3781978a",
   "metadata": {},
   "source": [
    "### Training and Inference on GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ab75a7",
   "metadata": {},
   "source": [
    "Using a graphical processing unit (GPU) can speed up training by several orders of magnitude. In PyTorch, it is very easy to take advantage of GPUs. We can define a `torch.device`, depending on the resources that are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f2e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab27449",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb9b722",
   "metadata": {},
   "source": [
    "*Implement the missing bits in  training loop (identified by comments):*\n",
    "* _Move data (images and labels) from the CPU to the `device`_\n",
    "* _Reset gradients of the optimizer_\n",
    "* _Perform forward pass_\n",
    "* _Compute the loss (tip: `torch.Tensor.item()` allows to get the element of a single-element tensor)_\n",
    "* _Perform bakcpropagation_\n",
    "* _Update the weights of the model using the optimizer_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed241b0",
   "metadata": {},
   "source": [
    "Some layers, such as `Dropout` and `BatchNorm2D` change behaviour from training to inference, and it is therefore very important to use `train()` and `eval()` member functions of `torch.nn.Model` to activate the correct behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef57686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_function, trainloader, epochs, device):\n",
    "    import time\n",
    "    from copy import deepcopy\n",
    "\n",
    "    # Move model to the device\n",
    "    # TODO\n",
    "\n",
    "    train_losses, valid_losses, accuracies = [], [], []\n",
    "    \n",
    "    best_valid_loss = np.inf\n",
    "\n",
    "    pbar = trange(epochs, desc='Training', leave=True)\n",
    "    for epoch in pbar:\n",
    "        \n",
    "        epoch_loss = 0\n",
    "\n",
    "        # Ensure model is in training mode\n",
    "        # TODO\n",
    "        \n",
    "        for images, labels in trainloader:\n",
    "\n",
    "            # Move data to GPU\n",
    "            # TODO\n",
    "            images, labels =\n",
    "        \n",
    "            # Initialize optimizer gradients to zero\n",
    "            # TODO\n",
    "            \n",
    "            # Perform forward pass\n",
    "            # TODO\n",
    "            \n",
    "            # Compute the loss\n",
    "            # TODO\n",
    "            \n",
    "            # Perform backpropagation\n",
    "            # TODO\n",
    "            \n",
    "            # Update model weights\n",
    "            # TODO\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        valid_loss = 0\n",
    "        accuracy = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Ensure model is in evaluation mode\n",
    "            # TODO\n",
    "            model.eval()\n",
    "\n",
    "            for images, labels in validloader:\n",
    "\n",
    "                # Move data to GPU\n",
    "                # TODO\n",
    "                images, labels =\n",
    "                    \n",
    "                # Perform forward pass\n",
    "                # TODO\n",
    "                output =\n",
    "                    \n",
    "                # Accumulate validation loss\n",
    "                valid_loss += loss_function(output, labels).item()\n",
    "                \n",
    "                # Compute class probabilities\n",
    "                # TODO\n",
    "                p =\n",
    "                    \n",
    "                # Compute accuracy\n",
    "                top_p, top_c = p.topk(1, dim=1) # Top prediction\n",
    "                equals = (top_c == labels.view(*top_c.shape)).type(torch.FloatTensor)\n",
    "                accuracy += torch.mean(equals)\n",
    "                    \n",
    "        train_losses.append(epoch_loss/len(trainloader))\n",
    "        valid_losses.append(valid_loss/len(validloader))\n",
    "        accuracies.append(accuracy.item()/len(validloader)*100)\n",
    "\n",
    "        # Save best model state dictionary\n",
    "        # Hint: remember to perform a deep copy\n",
    "        # TODO\n",
    "        if valid_losses[-1] < best_valid_loss:\n",
    "            best_state_dict =\n",
    "            best_valid_loss =\n",
    "                \n",
    "        pbar.set_postfix({\"Accuracy\": accuracies[-1]})\n",
    "\n",
    "    # Re-Load best model at the end of training\n",
    "    # TODO\n",
    "    \n",
    "    return train_losses, valid_losses, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c55ba6f",
   "metadata": {},
   "source": [
    "*Define the appropriate loss function, depending on your model output:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4841ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "nll_loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2211fff3",
   "metadata": {},
   "source": [
    "For optimization, we will use the `Adam` optimizer with the following learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a33214",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fdea3a",
   "metadata": {},
   "source": [
    "*Define and train three models:*\n",
    "* _Without data augmentation and with dropout layers (`15` epochs)_\n",
    "* _With dropout layers (dropout probability of `0.5`) and without data augmentation (`30` epochs)_\n",
    "* _With data agumentation and without dropout layers (`30` epochs)_\n",
    "\n",
    "*Do not forget to define the corresponding `Adam` optimizer for each model (with the same `learning_rate`).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a6c413-a292-4c58-bf41-acf3e87fa411",
   "metadata": {},
   "source": [
    "_Without data augmentation and with dropout layers (`15` epochs):_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b64a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "model =\n",
    "\n",
    "# TODO\n",
    "optimizer =\n",
    "\n",
    "# TODO\n",
    "train_losses, valid_losses, accuracies ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944ca6ea-125d-449a-ace0-50627c87f249",
   "metadata": {
    "tags": []
   },
   "source": [
    "* _With dropout layers (dropout probability of `0.5`) and without data augmentation (`30` epochs):_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd496b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "dmodel =\n",
    "\n",
    "# TODO\n",
    "doptimizer =\n",
    "\n",
    "# TODO\n",
    "d_train_losses, d_valid_losses, d_accuracies ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0517bf-9dd7-4c30-8a80-a043e72b3843",
   "metadata": {},
   "source": [
    "* _With data agumentation and without dropout layers (`30` epochs):_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f318aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "amodel =\n",
    "\n",
    "# TODO\n",
    "aoptimizer =\n",
    "\n",
    "# TODO\n",
    "a_train_losses, a_valid_losses, a_accuracies ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2d1f59",
   "metadata": {},
   "source": [
    "Once training is finished we can plot the loss of both the training and the validation sets as a function of the epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b173fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "ax1 = fig.add_subplot(1, 3, 1)\n",
    "ax1.plot(train_losses[1:], \"o-\", label=\"Train Loss\")\n",
    "ax1.plot(valid_losses, \"o-\", label=\"Valid Loss\")\n",
    "ax1.set_title(\"No Dropouts/No Agumentation\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "ax2 = fig.add_subplot(1, 3, 2, sharey=ax1)\n",
    "ax2.plot(d_train_losses[1:], \"o-\", label=\"Train Loss\")\n",
    "ax2.plot(d_valid_losses, \"o-\", label=\"Valid Loss\")\n",
    "ax2.set_title(\"Dropout\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.legend()\n",
    "ax3 = fig.add_subplot(1, 3, 3, sharey=ax1)\n",
    "ax3.plot(a_train_losses[1:], \"o-\", label=\"Train Loss\")\n",
    "ax3.plot(a_valid_losses, \"o-\", label=\"Valid Loss\")\n",
    "ax3.set_title(\"Data Augmentation\")\n",
    "ax3.set_xlabel(\"Epoch\")\n",
    "ax3.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e271379b",
   "metadata": {},
   "source": [
    "We notice immediately that regularisation reduces overfitting, and the validation loss does not start increasing early as without regulasiation. This means that the model is not over-fitting the training set, can be trained for longer, and generalises better. We can see that dropout layers and data agumentation have a similar effect on training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701078d7",
   "metadata": {},
   "source": [
    "## Evaluation of Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1770fd12",
   "metadata": {},
   "source": [
    "*Complete the missing steps needed to obtained the predicted labels and the ground truth for the whole of the test set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eb660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, testloader):\n",
    "    n = len(testloader) * 64\n",
    "    \n",
    "    y_pred = np.zeros(n)\n",
    "    y_true =np.zeros(n)\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # Ensure model is in evaluation mode\n",
    "    # TODO\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for images, labels in testloader:\n",
    "\n",
    "            # Move data to GPU\n",
    "            # TODO\n",
    "            images, labels =\n",
    "                \n",
    "            # Perform forward pass\n",
    "            # TODO\n",
    "            output =\n",
    "                \n",
    "            # Compute class probabilities\n",
    "            # TODO\n",
    "            p =\n",
    "                \n",
    "            # Get class of top prediction\n",
    "            # TODO\n",
    "            p, c =\n",
    "            \n",
    "            y_pred[i*64:i*64+64] = c.cpu().numpy()\n",
    "            y_true[i*64:i*64+64] = labels.cpu().numpy()\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "    assert i == len(testloader)\n",
    "    \n",
    "    return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b8a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_pred, l_true = predict(model, testloader)\n",
    "a_l_pred, a_l_true = predict(amodel, testloader)\n",
    "d_l_pred, d_l_true = predict(dmodel, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c61622",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(l_pred, l_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d9374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(a_l_pred, a_l_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec99718",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(d_l_pred, d_l_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadbe5c1-948e-49c8-afc8-c813caf4ad8d",
   "metadata": {},
   "source": [
    "*If there is time left, you can try to improve the model (make the model deeper, replace dropout layers with batch normalization, ...) in order to improve performance.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
