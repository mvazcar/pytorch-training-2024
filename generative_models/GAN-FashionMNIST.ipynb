{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network for FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the usual Python suspects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid surprises due to the random initialization of weights in our models, we set the random seed. See [PyTorch: Reproducibility](https://pytorch.org/docs/stable/notes/randomness.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed = 42\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to build a Generative Adversarial Network (GAN) to generate new images that look like the ones contained in the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Once you finish going throught this notebook, you can try to run the same notebook with the MNIST data set.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let's define the `torch.device` we will use for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the FashionMNIST data set. Since we are working with a generative model and trying to learn the underlying data distribution of the training set, we don't need to worry about the validation or test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Define a [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) from the `trainset` data set. Use the pre-defined `batch_size` (hyperparameter), and enable shuffling of the examples. Drop the last batch (with a different batch size), to make things easier (we hard-code `batch_size` in several places, and use it for assertions).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5)\n",
    "])\n",
    "\n",
    "trainset = datasets.FashionMNIST(root=\"data/\", train=True, download=True, transform=transform)\n",
    "\n",
    "# Define the trainloader using a PyTorch DataLoader\n",
    "# TODO\n",
    "trainloader =\n",
    "\n",
    "trainiter = iter(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good idea to have a look at the data. Let's plot a bunch of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_to_name = { \n",
    "    i : name \n",
    "    for i, name in enumerate([\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]) \n",
    "}\n",
    "\n",
    "images, labels = next(trainiter)\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "for idx in range(64):\n",
    "    ax = fig.add_subplot(8, 8, idx + 1, xticks=[], yticks=[])\n",
    "\n",
    "    img = images[idx].numpy()\n",
    "    \n",
    "    img = img * 0.5 + 0.5 \n",
    "\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    \n",
    "    plt.imshow(img, cmap='gray')\n",
    "    \n",
    "    name = str(label_to_name[labels[idx].item()])\n",
    "    \n",
    "    ax.set_title(name, fontdict={\"fontsize\": 12})\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On simple toy examples such as MNIST, we can get reasonable results already with fully connected generator and discriminator networks (with dropout layers and leaky ReLU activation functions). _Try it!_ However, in order to get good results, we are implementing here a deep convolutional GAN (DCGAN) model, using `nn.Conv2D` and `nn.Conv2DTranspose` layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminator network $D(x)$ takes an image $x$ as input and outputs the probability of $x$ being a real image (coming from the training data), rather thank a fake (generated) image. It is therefore a binary classifier based on 2D convolutions, like we have seen before. However, since we are working with binary classification, we will produce a single output (the probability $p$ of the image being real), instead of the two class probabilities. The probability of the image being fake is simply $1-p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the architecture is a bit different from the one we have seen before (with `nn.MaxPool` for downsampling and `nn.Linear` for classification). GANs are notoriously difficult to train, as we discussed. Therefore, we follow the architrcture guidelines from the [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/pdf/1511.06434.pdf) paper:\n",
    "* Replace pooling layers with strided convolutions\n",
    "* Use batch normalisation\n",
    "* Remove fully connected hidden layers\n",
    "* Use `nn.LeakyReLU` activation for the discriminator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to remove pooling layer, we can choose the stride so that the spatial dimension is divided by 2 at each convolution. In order to remove the final fully connected layer(s), we need to set the `kernel_size`, `stride`, and `padding` of the last convolutional layer in such a way that the there is a single number as output (the probability of a real image). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Given the layers provided below, implement the forward pass.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lrelu = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(256, 1, kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # First convolution and activation\n",
    "        # TODO\n",
    "        \n",
    "        assert x.shape == (batch_size, 64, 14, 14), x.shape\n",
    "        \n",
    "        # Second convolution, batch normalisation, and activation\n",
    "        # TODO\n",
    "\n",
    "        assert x.shape == (batch_size, 128, 7, 7), x.shape\n",
    "        \n",
    "        # Third convolution, batch normalisation, and activation\n",
    "        # TODO\n",
    "\n",
    "        assert x.shape == (batch_size, 256, 3, 3), x.shape\n",
    "\n",
    "        # Last convolution\n",
    "        # TODO\n",
    "\n",
    "        assert x.shape == (batch_size, 1, 1, 1), x.shape\n",
    "        \n",
    "        # Reshape to have a single output (per batch)\n",
    "        # TODO\n",
    "        x = \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that we can propagate a batch of images through the discriminator and get an output with the correct shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Discriminator()\n",
    "D(torch.ones((batch_size, 1, 28, 28))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator network $G(z)$ takes a latent space sample $z$ (random noise) and ouputs an image. The goal of the generator is to approximate the data distribution $p_\\text{data}$, so that output images are similar to the ones in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the architrcture guidelines from the [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/pdf/1511.06434.pdf) paper:\n",
    "* Use transpose convolutions\n",
    "* Use batch normalisation\n",
    "* Remove fully connected hidden layers\n",
    "* Use `nn.ReLU` activation for the generator (`nn.Tanh` for the last layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Given the layers defined below, implement the forward pass.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, z_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.z_size = z_size\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "        self.conv1 = nn.ConvTranspose2d(100, 512, 4, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv2 = nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv3 = nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.ConvTranspose2d(128, 1, 2, stride=2, padding=2, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Re-shape input\n",
    "        # The latent space vector is interpreted as feature maps of size 1x1\n",
    "        # TODO\n",
    "        x =\n",
    "        \n",
    "        # First convolution, batch normalisation, and activation\n",
    "        # TODO\n",
    "        \n",
    "        assert x.shape == (batch_size, 512, 4, 4), x.shape\n",
    "        \n",
    "        # Second convolution, batch normalisation, and activation\n",
    "        # TODO\n",
    "\n",
    "        assert x.shape == (batch_size, 256, 8, 8), x.shape\n",
    "        \n",
    "        # Third convolution, batch normalisation, and activation\n",
    "        # TODO\n",
    "\n",
    "        assert x.shape == (batch_size, 128, 16, 16), x.shape\n",
    "        \n",
    "        # Last convolution\n",
    "        # TODO\n",
    "        \n",
    "        assert x.shape == (batch_size, 1, 28, 28), x.shape\n",
    "        \n",
    "        # tanh activation function\n",
    "        # TODO\n",
    "        x =\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we can generate an image of the correct size from a latent space vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(100)\n",
    "G(torch.ones((batch_size, 100))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined the architecture of the generator and discriminator networks, we can create an instance of both. We also define the size of the latent space representation $z$. This is yet another hyperparameter of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator latent vector size\n",
    "z_size = 100\n",
    "\n",
    "D = Discriminator().to(device)\n",
    "G = Generator(z_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two losses, for convenience. The losses take the output of the discriminator (probability of images being real). In the `real_loss` the labels are all set to real images, while in the `fake_loss` the labels are all set to fake images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Since the discriminator has a simple output, we need to use the `binary_cross_entropy_with_logits` loss, in order to compute the (binary) cross-entropy loss with the raw output of the model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_loss(D_out, device=device):\n",
    "    # The labels should represent real images (for the discriminator)\n",
    "    # Hint: use torch.ones or torch.zeros and the correct batch size\n",
    "    # TODO\n",
    "    labels =\n",
    "            \n",
    "    # Compute the binary cross entropy loss\n",
    "    # TODO\n",
    "    loss =\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_loss(D_out, device=device):\n",
    "    # The labels should represent fake images (for the discriminator)\n",
    "    # Hint: use torch.ones or torch.zeros and the correct batch size\n",
    "    # TODO\n",
    "    labels =\n",
    "    \n",
    "    # Compute the binary cross entropy loss\n",
    "    # TODO\n",
    "    loss =\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the oprimizer, we use again the [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer. Since we are trying to train two models at the same time,  we need two optimisers (one for each model). For simplicity, we use the same learning rate and momentum. $\\beta_1=1/2$ is set according to the DCGAN paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "lr = 1e-3\n",
    "\n",
    "# Create optimizers for discriminator and generator\n",
    "d_optimizer = optim.Adam(D.parameters(), lr, betas=(0.5, 0.999))\n",
    "g_optimizer = optim.Adam(G.parameters(), lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the DCGAN paper, all model weights are randomly initialized from a normal distribution with mean `0`, and standard deviation `0.02`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    layer = m.__class__.__name__\n",
    "    if layer.startswith('Conv'): # Both Conv and ConvTranspose\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif layer.startswith('BatchNorm'):\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Apply the custom weight initialisation to both the discriminator and the generator.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training we will save some generated samples, in order to visualize how the training progressed ath the end. Therefore, we define a batch of fixed latent space vectors $z$ that we re-use each time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = batch_size # Remvoe assertions in generator to make this work with different sizes\n",
    "\n",
    "fixed_z = torch.randn(sample_size, z_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can write the training loop. As we have seen, the training proceeds in two phases for each mini-batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Implement these phases in the training loop._\n",
    "\n",
    "**_Discriminator training_**:\n",
    " * _Apply the discriminator to real images and compute the (real) loss_\n",
    " * _Generate  a batch of fake images (sample $z$, then compute $G(z)$_\n",
    " * _Apply the dirscriminator to real images and compute the (fake) loss_\n",
    " * _Compute the total loss by summing the two losses above_\n",
    "\n",
    "\n",
    "**_Generator training_**:\n",
    " * _Generate  a batch of fake images (sample $z$, then compute $G(z)$_\n",
    " * _Apply the dirscriminator to fake images and compute the (real) loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clever bit of GANs is in the last bullet point: when training the generator, we pretend that the generated images are real when we pass them to the discriminator. Since we are only updating the weights of the generator, the only way to improve this loss if for the generator to generate better images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import trange, tqdm\n",
    "\n",
    "n_epochs = 15\n",
    "\n",
    "# Keep track of loss and generated, \"fake\" samples\n",
    "samples = []\n",
    "losses = []\n",
    "\n",
    "# Models in train model (for dropout layers)\n",
    "D.train()\n",
    "G.train()\n",
    "\n",
    "iters = 0\n",
    "pbar = trange(n_epochs, desc='Training', leave=True)\n",
    "for epoch in pbar:\n",
    "    \n",
    "    for real_images, _ in trainloader:\n",
    "        \n",
    "        real_images = real_images.to(device)\n",
    "                \n",
    "        # @@@@@@@@@@@@@@@@@@@@@@\n",
    "        # DISCRIMINATOR TRAINING\n",
    "        # @@@@@@@@@@@@@@@@@@@@@@\n",
    "        \n",
    "        d_optimizer.zero_grad()\n",
    "\n",
    "        # Discriminator predictions and loss for real images\n",
    "        # TODO\n",
    "        D_real =\n",
    "        d_real_loss =l)\n",
    "        \n",
    "        # Generate fake images\n",
    "        # TODO\n",
    "        z =\n",
    "        fake_images =\n",
    "        \n",
    "        # Discriminator predictions and loss for fake images\n",
    "        # TODO\n",
    "        D_fake =\n",
    "        d_fake_loss =\n",
    "        \n",
    "        # Total discriminator loss\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        # TODO\n",
    "\n",
    "        # Update weights of the discriminator\n",
    "        # TODO\n",
    "        \n",
    "        # @@@@@@@@@@@@@@@@@@\n",
    "        # GENERATOR TRAINING\n",
    "        # @@@@@@@@@@@@@@@@@@\n",
    "        \n",
    "        g_optimizer.zero_grad()\n",
    "        \n",
    "        # Generate fake images\n",
    "        # TODO\n",
    "        z = \n",
    "        fake_images =\n",
    "        \n",
    "        # Discriminator predictions and loss for fake images\n",
    "        # This now uses real_loss instead of fake_loss\n",
    "        # in order to flip the labels\n",
    "        # TODO\n",
    "        D_fake =\n",
    "        g_loss =\n",
    "        \n",
    "        # Backpropagation\n",
    "        # TODO\n",
    "        \n",
    "        # Update weights of the generator\n",
    "        # TODO\n",
    "        \n",
    "        # @@@@@@@@@@@@@@@@@@\n",
    "        \n",
    "        # Store losses\n",
    "        losses.append((d_loss.item(), g_loss.item()))\n",
    "        \n",
    "        if iters % 250 == 0:\n",
    "            # Generate fake images from fixed sample\n",
    "            G.eval()\n",
    "            with torch.no_grad():\n",
    "                samples.append(G(fixed_z))\n",
    "            G.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses[:,0], \"o-\", label='Discriminator')\n",
    "plt.plot(losses[:,1], \"o-\", label='Generator')\n",
    "plt.title(\"Training Losses\")  \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the generator to generate new images.Generate new latent space samples $z$ and use the generator $G(z)$ to generate new images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Generate new latent space samples $z$ and use the generator $G(z)$ to generate new images.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Generate images\n",
    "    # TODO\n",
    "    z =\n",
    "    images =\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "for idx in range(64):\n",
    "    ax = fig.add_subplot(8, 8, idx + 1, xticks=[], yticks=[])\n",
    "\n",
    "    img = images[idx].detach().cpu().numpy()\n",
    "    \n",
    "    img = img * 0.5 + 0.5\n",
    "    \n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    \n",
    "    ax.imshow(img, cmap='gray')\n",
    "    \n",
    "    # Get name\n",
    "    name = str(labels[idx].item())\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the generated images are pretty decent, compared to the real ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we stored some samples during training, we can also visualise how the generator evolves during training. On order to show more examples at the beginning of training, we use `np.logspace` to generate the indices of the time steps we want to visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = np.logspace(0, np.log10(len(samples) - 1), num=10, dtype=int)\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = len(ss)\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "for row, s in enumerate(ss):\n",
    "    for col in range(10):\n",
    "        ax = fig.add_subplot(n, 10, col + 10 * row + 1, xticks=[], yticks=[])\n",
    "\n",
    "        img = samples[s][col].detach().cpu().numpy()\n",
    "        \n",
    "        img = img * 0.5 + 0.5\n",
    "\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        \n",
    "        ax.imshow(img, cmap='gray')\n",
    "\n",
    "        # Remove axes\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
