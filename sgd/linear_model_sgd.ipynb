{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of a Simple Stochastic Gradient Descent\n",
    "\n",
    "Here we visualize the minimization of the loss with the SGD algorithm in its variants vanilla GD, batch SGD and minibatch SGD. For this we consider a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Let's create a [PyTorch dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
    "We generate a random vector $x \\in [-0.5, 0.5]$ and evaluate it in a linear function $y = 2x$ (we set `n = 0`).\n",
    "We add some noise to $y$ and that gives us $y \\in [-1.1, 1.1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDataset(Dataset):\n",
    "    '''The training data is generated from the linear\n",
    "    function\n",
    "              y = m * x + n  \n",
    "    where `m` is the slope and `n` is the offset.\n",
    "    Random noise in the range `[-0.1, 0.1]` is added\n",
    "    to the function value `y`.\n",
    "    '''\n",
    "    slope = 2.0\n",
    "    offset = 0.0\n",
    "    nsamples = 1500\n",
    "\n",
    "    # A dedicated random number generator for the dataset\n",
    "    rnd_gen_data = torch.Generator().manual_seed(1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(1,).uniform_(-0.5, 0.5, generator=self.rnd_gen_data)\n",
    "        y = self.slope * x + self.offset\n",
    "        noise = torch.FloatTensor(1,).uniform_(-0.1, 0.1, generator=self.rnd_gen_data)\n",
    "        return (x, y + noise)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = LinearDataset()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how the batches look like\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = []\n",
    "y_plot = []\n",
    "for x, y in train_loader:\n",
    "    x_plot.append(x)\n",
    "    y_plot.append(y)\n",
    "    \n",
    "x_plot = torch.stack(x_plot, dim=1).flatten()\n",
    "y_plot = torch.stack(y_plot, dim=1).flatten()\n",
    "\n",
    "plt.plot(x_plot, y_plot, '.')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining the model, a loss function and an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a model: \n",
    "# linear model(x) = m x + n\n",
    "model = torch.nn.Linear(1, 1, device=device)\n",
    "\n",
    "# Select a loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Select an optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(model, loss):\n",
    "    \"\"\"Utility function for plotting\"\"\"\n",
    "\n",
    "    return(model.weight.item(),\n",
    "           model.bias.item(),\n",
    "           loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "history = []\n",
    "\n",
    "for epoch in range(num_epochs):                 #   loop over epochs:\n",
    "    # set a seed at each epoch to always        #\n",
    "    # generate the same set of random x         #\n",
    "    train_set.rnd_gen_data.manual_seed(1)       #\n",
    "    #                                           #\n",
    "    for x, y in train_loader:                   #      loop over batches:  -> (x, y)\n",
    "        optimizer.zero_grad()                   #         * reset automatic differentiation record\n",
    "        y_hat = model(x.to(device))             #         * evaluate the model in a batch -> y_hat (forward pass)\n",
    "        loss = loss_fn(y_hat, y.to(device))     #         * evaluate the loss function with the obtained y_hat and y\n",
    "        history.append(log(model, loss))        #         [not part of the traing] keep values for plotting later\n",
    "        loss.backward()                         #         * backpropagation -> gradients\n",
    "        optimizer.step()                        #         * update weights with the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_hist  = np.array(history)[:, 0]\n",
    "offset_hist = np.array(history)[:, 1]\n",
    "loss_hist   = np.array(history)[:, 2]\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 4)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_hist, 'r-')\n",
    "plt.xlabel('Training steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_plot, y_plot, '.', alpha=.1)\n",
    "plt.plot(x_plot, slope_hist[0]  * x_plot + offset_hist[0],  'r-', label='model (initial step)', lw=3)\n",
    "plt.plot(x_plot, slope_hist[-1] * x_plot + offset_hist[-1], 'b-', label='model (trained)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_field(m, n, xref, yref):\n",
    "    '''Utility function for ploting the loss'''\n",
    "    return np.mean(np.square(yref - m * xref - n ))\n",
    "\n",
    "_m = np.arange( -0.5, 4.51, 0.1)\n",
    "_n = np.arange(-1., 1.01, 0.1)\n",
    "M, N = np.meshgrid(_m, _n)\n",
    "\n",
    "Z = np.zeros(M.shape)\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        Z[i, j] = loss_function_field(M[i, j], N[i, j],\n",
    "                                      x_plot.numpy(), y_plot.numpy())\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (16.0, 5.2)\n",
    "\n",
    "cp = plt.contour(M, N, Z, 16, vmin=Z.min(), vmax=Z.max(), alpha=0.99, colors='k', linestyles=':', linewidths=0.8)\n",
    "plt.contourf(M, N, Z, 60, vmin=Z.min(), vmax=Z.max(), alpha=0.2, cmap='Blues')  #plt.cm.RdYlBu_r)\n",
    "plt.clabel(cp, cp.levels[:6])\n",
    "plt.colorbar()\n",
    "m = slope_hist[-1]\n",
    "n = offset_hist[-1]\n",
    "plt.plot([train_set.slope], [train_set.offset], 'rx', ms=10)\n",
    "plt.plot(slope_hist, offset_hist, '.-', lw=2, c='k')\n",
    "plt.xlim([_m.min(), _m.max()])\n",
    "plt.ylim([_n.min(), _n.max()])\n",
    "plt.xlabel('Slope')\n",
    "plt.ylabel('Offset')\n",
    "plt.show()\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (6, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Try different batch sizes and see the effect path of the SGD:\n",
    " * `batch_size = 1000` - The whole dataset\n",
    " * `batch_size = 128`  - Something between 1 and the size of the whole dataset\n",
    " * `batch_size = 1`    - This is perhaps too slow. Maybe try a smaller batch size, such as 16\n",
    "\n",
    "2. Why is the training with `batch_size = 1` so slow compared to `batch_size = 128`?\n",
    "3. Try different learning rates and see how the path of the SGD looks like."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2024",
   "language": "python",
   "name": "ml2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
