{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ed091b-e0c4-44f7-a41e-9c9e1c6f25c4",
   "metadata": {},
   "source": [
    "# The Multilayer Perceptron\n",
    "\n",
    "In deep learning, one of the fundamental architectures is the multilayer perceptron (MLP), serving as a cornerstone for more complex neural networks.\n",
    "\n",
    "At its core, a MLP consists of multiple layers of neurons, each layer connected to the next in a feedforward manner. These layers typically include an input layer, one or more hidden layers, and an output layer. The magic lies in the interconnectedness of these layers and the activation functions applied at each step.\n",
    "\n",
    "The following image shows a schematic representation of an MLP for regression tasks where models usually have a linear layer in the output layer, allowing it to predict continuous numeric values.\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"imgs/mlp-classification.svg\" alt=\"MLP for classification\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d874c513-6970-44ad-bce4-66555ae31a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb6f5b4-7e3d-4066-b0d1-3d87e212f227",
   "metadata": {},
   "source": [
    "Let's consider a simple MLP model for a classification of `(28, 28)` images in 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c80ff4-1be8-4816-88e3-851cedd139a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch of synthetic images\n",
    "x = torch.randn((8, 28, 28))   # (batch_size, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581934cc-9899-4c94-8af7-bf1c88a4c3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of MLP for an image classification problem\n",
    "class MLPClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)   # (28 * 28, 128)  # same size of the input images\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)      # Flatten input images            (batch_size, 28, 28) -> (batch_size, 784)\n",
    "        x = self.fc1(x)          # Apply Fully Connected layer 1   (batch_size, 784)    -> (batch_size, 128)\n",
    "        x = self.relu1(x)        # Apply ReLU activation Function  (batch_size, 784)\n",
    "        x = self.fc2(x)          # Apply Fully Connected layer 2   (batch_size, 128)    -> (batch_size, 64)\n",
    "        x = self.relu2(x)        # Apply ReLU activation Function  (batch_size, 64)\n",
    "        x = self.fc3(x)          # Apply Fully Connected layer 3   (batch_size, 64)     -> (batch_size, 10)\n",
    "        x = self.softmax(x)      # Apply Softmax function          (batch_size, 10)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5cd26-d594-4d47-b09f-55568c6477b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassification()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7101c71-00bb-4f1e-9dd8-e2cd2f68c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2682c1-976c-4ff6-801c-1ed74fbfd697",
   "metadata": {},
   "source": [
    "### Operations in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad9913d-0e9c-41a1-87f1-2f88aa65cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. \n",
    "# x = x.view(-1, 784)  # Flatten input images from (batch_size, 28, 28) to (batch_size, 784)\n",
    "#\n",
    "x_flat = x.view(-1, 784)\n",
    "x_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7af31f-cf25-4d8b-a925-c5d8d13860b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "# x = self.fc1(x)  # Apply Fully Connected layer 1   (batch_size, 784)   -> (batch_size, 128)\n",
    "#\n",
    "lin = nn.Linear(784, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf42eb76-e861-4610-b917-cdf77a8bea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin(x_flat).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087f7a0c-9bab-4e02-b070-f5c03b271d2e",
   "metadata": {},
   "source": [
    "Internally linear layers have two sets of parameters: the weights matrix $W$ and the biasses $\\vec{b}$. Applying a linear layer means doing a linear transformation $\\vec{x}^* = W\\vec{x} + \\vec{b}$.\n",
    "\n",
    "We can access the internals of the layer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1aff50-4f64-4e78-aece-2bbf989a9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f325ce37-bd0a-448b-8afa-90519a4abad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c9482d-25da-425d-99b3-4bb639c13cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the linear transformation `x^* = Wx + b` by hand\n",
    "# for one sample in the batch: x_flat[0]\n",
    "#\n",
    "x_flat_star = x_flat[0] @ lin.weight.T + lin.bias\n",
    "\n",
    "x_flat_star.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49941a5-a71a-4acc-aacd-592e7fd34588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing with the linear layer\n",
    "torch.allclose(lin(x_flat[0]), x_flat_star) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2024",
   "language": "python",
   "name": "ml2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
