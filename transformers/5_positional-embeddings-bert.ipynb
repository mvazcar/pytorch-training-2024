{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "380478ca-79e3-49a0-8676-ee8cf2dea112",
   "metadata": {},
   "source": [
    "# BERT Step by Step: Positional Embeddings\n",
    "\n",
    "In natural language processing, positional embeddings play a crucial role in understanding the sequential nature of language data. Word embeddings capture semantic relationships between words but lack the ability to encode sequential information.\n",
    "\n",
    "Positional embeddings complement word embeddings by encoding the position or order of words in a sequence. They provide a way for models to differentiate between words based not only on their meanings but also on their positions within the input sequence.\n",
    "\n",
    "* [*Yu-An Wang, Yun-Nung Chen*. What Do Position Embeddings Learn?An Empirical Study of Pre-Trained Language Model Positional Encoding](https://arxiv.org/abs/2010.04903)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef9a343-af66-40c2-bbb2-ae0473c41e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from transformers import BertForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cd3506-1e3c-4cf0-8663-e000df0a6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6d8daa-9f35-485e-b07b-c8e62060640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForPreTraining.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87b5c19-faa4-47e5-bb34-f4d607ba26fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"let's tokenize something?\", return_tensors=\"pt\")\n",
    "seq_embedding = model.bert.embeddings.word_embeddings(encoding)\n",
    "seq_embedding.shape   # (batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6335eded-6900-447f-a760-c0693689b37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.bert.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7eb092-f6c8-45f1-b960-bf49bb7fad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.hidden_size              # size of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa73fad3-eb6c-45c3-8efc-278e8ecd4af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.max_position_embeddings  # max seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af95431c-e791-44df-9037-e12a5f41b2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = encoding.shape[-1]\n",
    "\n",
    "# Array with the postions for our sequence\n",
    "pos_embedding_seq = model.bert.embeddings.position_embeddings.weight[:seq_len]\n",
    "\n",
    "# Reshape as a batch of 1\n",
    "pos_embedding_seq.view((1, seq_len, config.hidden_size)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb3c808-bc75-417b-adc0-b63d1da9404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_embedding + pos_embedding_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99022d60-d942-495c-adbd-c3f9af200148",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (12, 1)\n",
    "\n",
    "for i in [0, 1, 2, 10, 100, 200, 300, 400, 500]:\n",
    "    plt.plot(model.bert.embeddings.position_embeddings.weight.detach().numpy()[i],    alpha=0.5, c='red')\n",
    "    plt.plot(seq_embedding[0, 2].detach().numpy(), alpha=0.5, c='blue')\n",
    "    plt.xlim([0, config.hidden_size])\n",
    "    plt.ylim([-0.15, 0.15])\n",
    "    plt.show()\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bfc6c5-139d-45b3-bcbd-4564d5e39c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = cosine_similarity(model.bert.embeddings.position_embeddings.weight.detach().numpy())\n",
    "plt.matshow(similarity_matrix, cmap='Blues')\n",
    "plt.ylabel('Position')\n",
    "plt.xlabel('Position')\n",
    "plt.gca().xaxis.tick_top()\n",
    "plt.gca().xaxis.set_label_position('top') \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2024",
   "language": "python",
   "name": "ml2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
