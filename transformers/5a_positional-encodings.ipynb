{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "380478ca-79e3-49a0-8676-ee8cf2dea112",
   "metadata": {},
   "source": [
    "# Sinusoidal Positional Encodings\n",
    "\n",
    "In natural language processing, positional embeddings play a crucial role in understanding the sequential nature of language data. Word embeddings capture semantic relationships between words but lack the ability to encode sequential information.\n",
    "\n",
    "Positional embeddings complement word embeddings by encoding the position or order of words in a sequence. They provide a way for models to differentiate between words based not only on their meanings but also on their positions within the input sequence.\n",
    "\n",
    "One popular approach for generating positional embeddings is through the use of sinusoidal functions, as introduced in the Transformer architecture. These embeddings consist of sine and cosine functions of different frequencies and phases, allowing the model to learn unique representations for each position in the input sequence.\n",
    "\n",
    "* [*Yu-An Wang, Yun-Nung Chen*. What Do Position Embeddings Learn?An Empirical Study of Pre-Trained Language Model Positional Encoding](https://arxiv.org/abs/2010.04903)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef9a343-af66-40c2-bbb2-ae0473c41e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from transformers import T5ForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cd3506-1e3c-4cf0-8663-e000df0a6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 't5-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6d8daa-9f35-485e-b07b-c8e62060640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ae5225-b486-4d30-8801-ee7c088137d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.n_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3147328-b365-4900-a83e-632286fb210a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96b7b9-8904-49cf-b7b0-05c98ea0833d",
   "metadata": {},
   "source": [
    "**No positional embeddings!** T5 uses sinusoidal positional encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4824ec-e40b-479b-9d33-41e50acfd73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_positional_encoding(max_len, d_model):\n",
    "    position = torch.arange(0, max_len)[:, None]\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "    pos_enc = torch.zeros((max_len, d_model))\n",
    "\n",
    "    pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "    pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    return pos_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043327bd-638f-4de5-9f76-de50edf7fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.max_position_embeddings = config.task_specific_params['translation_en_to_de']['max_length']\n",
    "config.hidden_size = config.d_model\n",
    "\n",
    "sin_pos_encoding = generate_positional_encoding(config.max_position_embeddings, config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89560c6a-1d28-4a14-a140-906123099232",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (12, 1)\n",
    "\n",
    "for i in [0, 1, 2, 10, 50, 100, 150, 200, 250, 299]:\n",
    "    plt.plot(sin_pos_encoding[i], c='blue')\n",
    "    plt.xlim([0, config.hidden_size])\n",
    "    plt.ylim([-1.5, 1.5])\n",
    "    plt.show()\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77db19ae-f78e-4fdc-a280-49ef2ba05e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sin_pos_encoding, cmap='Blues')\n",
    "plt.xlabel('Embedding Dimensions')\n",
    "plt.ylabel('Position in Sequence')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887e484c-3ca7-4a5d-8dc6-78a6d99eca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = cosine_similarity(sin_pos_encoding)\n",
    "plt.matshow(similarity_matrix, cmap='Blues')\n",
    "plt.ylabel('Position')\n",
    "plt.xlabel('Position')\n",
    "plt.gca().xaxis.tick_top()\n",
    "plt.gca().xaxis.set_label_position('top') \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2024",
   "language": "python",
   "name": "ml2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
